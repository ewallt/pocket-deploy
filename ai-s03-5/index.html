<!-- /pocket-deploy/ai-s03-5/index.html -->
<!DOCTYPE html>
<html lang="en">
<head><meta charset="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/>
<title>AI s03 — Part 5: Toward Video & 3D</title>
<style>
:root{--bg:#0b1020;--panel:#0f172a;--ink:#e5e7eb;--muted:#94a3b8;--brand:#60a5fa;--border:#1f2940}
@media(prefers-color-scheme:light){:root{--bg:#f5f7fb;--panel:#fff;--ink:#1b2a41;--muted:#34495e;--brand:#2563eb;--border:#e3e9f3}}
*{box-sizing:border-box}body{margin:0;background:var(--bg);color:var(--ink);font:16px/1.6 system-ui,-apple-system,Segoe UI,Inter,Arial;padding:2rem}
.c{max-width:880px;margin:0 auto;background:var(--panel);border:1px solid var(--border);border-radius:14px;box-shadow:0 6px 24px rgba(0,0,0,.25)}
.nav{display:flex;gap:.5rem;flex-wrap:wrap;padding:.75rem;border-bottom:1px solid var(--border)}.nav a{padding:.45rem .7rem;border:1px solid var(--border);border-radius:999px;text-decoration:none;color:var(--ink)}.nav a:hover{border-color:var(--brand);color:var(--brand)}
.h{padding:1.25rem 1.5rem}.intro{color:var(--muted);font-style:italic;margin:.25rem 0 1rem}
.p{padding:0 1.5rem 1.4rem}.p p{margin:1.05rem 0}.f{padding:1rem 1.5rem;border-top:1px solid var(--border);color:var(--muted);font-size:.95rem}
table{width:100%;border-collapse:collapse;margin-top:.5rem}th,td{border:1px solid var(--border);padding:.4rem;text-align:left}th{color:var(--brand)}
</style></head>
<body><article class="c">
<nav class="nav">
  <a href="/pocket-deploy/dashboard-ai-s03/">← Dashboard</a>
  <a href="/pocket-deploy/ai-s03-4/">Previous</a>
</nav>
<header class="h"><h1>Toward Video & 3D — Part 5</h1><p class="intro">No, diffusion stopped being about stillness; it became temporal and spatial, extending imagination across time and depth.</p></header>
<section class="p">
<p>Context: having mastered still images, researchers turned to temporal coherence. Each frame of a video must relate to the next, yet remain diverse. Early frame-by-frame diffusion produced flicker; the solution was to treat time as another dimension of the latent field. Models like Imagen Video, Phenaki, and Pika extended text-to-image into text-to-video by enforcing motion consistency during denoising. This preserved semantics while enabling scene evolution.</p>
<p>Sequential explanation tracks the dimensional climb. Step 1: 2D diffusion with temporal conditioning; Step 2: latent motion vectors guiding frame transitions; Step 3: 3D neural fields (NeRF, Gaussian Splatting) representing volumetric scenes. The line between image and simulation blurred—diffusion became a general generative engine for physical coherence. Visual storytelling entered the domain once occupied by film editing and game rendering pipelines.</p>
<p>Parallel comparisons clarify progress. Traditional video synthesis stitched interpolations; new diffusion models generated scenes from scratch with dynamic lighting and consistent identity. While GAN videos often looped or melted, diffusion preserved causality and scene memory. In 3D, differentiable rendering and inverse graphics replaced polygon meshes with neural radiance, unifying perception and generation once again.</p>
<section>
<h3>Sectional Synthesis — From Frame to World</h3>
<table>
<tr><th>Stage</th><th>Focus</th><th>Innovation</th></tr>
<tr><td>2D Diffusion</td><td>Still images</td><td>Noise reversal</td></tr>
<tr><td>Video Diffusion</td><td>Temporal dynamics</td><td>Motion conditioning</td></tr>
<tr><td>3D Generative</td><td>Spatial continuity</td><td>Neural fields, splatting</td></tr>
</table>
</section>
<p>Conclusion: by 2023, diffusion had evolved from art generator to simulation framework. The generative frontier now spanned four dimensions—width, height, depth, and time—pointing toward immersive models where understanding and creation share the same latent space.</p>
</section>
<footer class="f">Sources note: Imagen Video, Phenaki, NeRF, Gaussian Splatting, and 3D-aware diffusion research 2022–2023.</footer>
</article></body></html>
