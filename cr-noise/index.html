<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Course Template</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.tailwindcss.com/3.4.1?plugins=typography"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Merriweather:wght@400;700&family=Lato:wght@400;700&display=swap" rel="stylesheet">
    <style>
        /* Default Theme: Newspaper */
        :root {
            --bg-gradient: #fdfdf8;
            --text: #1c1c1c;
            --accent-text: #8c1c13;
            --nav-bg: #f1f1eb;
            --nav-border: #dcdcd3;
            --nav-text: #1c1c1c;
            --nav-hover-bg: #e5e5dd;
            --nav-active-bg: #dcdcd3;
            --nav-active-text: #1c1c1c;
            --prose-text: var(--text);
            --prose-headings: var(--accent-text);
            --prose-links: var(--accent-text);
            --prose-bold: var(--text);
            font-family: 'Merriweather', serif;
        }

        .theme-emerald-paper, .theme-cyber-blue, .theme-emerald-gold {
            font-family: 'Lato', sans-serif;
        }
        
        body {
            background: var(--bg-gradient);
            color: var(--text);
            min-height: 100vh;
        }

        .ui-bg { background-color: var(--nav-bg); }
        .ui-border { border-color: var(--nav-border); }
        .text-primary { color: var(--text); }
        .text-accent { color: var(--accent-text); }

        .tab-button {
            background-color: var(--nav-bg);
            color: var(--nav-text);
            border-bottom: 3px solid transparent;
        }
        .tab-button:hover, .tab-button:focus-visible {
            background-color: var(--nav-hover-bg);
        }
        .tab-button.tab-active {
            background-color: var(--nav-active-bg);
            color: var(--nav-active-text);
            border-bottom-color: var(--accent-text);
        }

        .settings-button { background-color: var(--nav-hover-bg); }
        .settings-button:hover { opacity: 0.8; }
        *:focus-visible { outline: 3px solid var(--accent-text); outline-offset: 2px; }

        .prose { color: var(--prose-text); }
        .prose h2, .prose h3, .prose a, .prose strong { color: var(--prose-headings); }
        .prose p.lead { color: var(--prose-text); }
    </style>
</head>
<body class="transition-colors duration-300">
    <div class="container mx-auto px-4 py-6 max-w-7xl">
        <header id="app-header" class="ui-bg border ui-border rounded-lg shadow-lg p-6 mb-6"></header>
        <nav id="app-tabs" class="ui-bg border ui-border rounded-lg shadow-lg mb-6" role="tablist" aria-label="Main navigation"></nav>
        <main id="app-panels" class="ui-bg border ui-border rounded-lg shadow-lg p-6 md:p-8"></main>
        
        <div id="settingsModal" class="fixed inset-0 bg-black bg-opacity-50 hidden z-50 flex items-center justify-center p-4" role="dialog" aria-modal="true" aria-labelledby="settings-title">
            <div class="ui-bg border ui-border rounded-lg p-6 max-w-md w-full shadow-2xl">
                <h2 id="settings-title" class="text-xl font-bold text-accent mb-4">Display Settings</h2>
                <div class="space-y-4">
                    <div>
                        <label for="themeSelect" class="block text-primary font-semibold mb-2">Theme</label>
                        <select id="themeSelect" class="w-full px-3 py-2 rounded ui-bg border ui-border text-primary">
                        </select>
                    </div>
                </div>
                <div class="flex justify-end space-x-3 mt-6">
                    <button id="closeSettings" class="px-4 py-2 rounded text-primary hover:opacity-80">Close</button>
                </div>
            </div>
        </div>
    </div>

    <!-- 
    ============================================================
    vvvvvvvvvvvvvv PASTE YOUR JSON CONTENT BELOW vvvvvvvvvvvvvv
    ============================================================
    -->
    <script type="application/json" id="app-data">
{
  "decision-hygiene-course": {
    "title": "Decision Hygiene: A Course in Noise Reduction",
    "description": "An 8-unit course exploring the psychology of inconsistency and providing practical tools to measure and reduce noise in professional judgments.",
    "categories": {
      "unit-1-what-is-noise": {
        "display_order": 10,
        "name": "Unit 1: What Is Noise?",
        "display_name": "Unit 1",
        "paragraphs": [
          {
            "heading": "Noise vs. Bias: The Missing Half of Error",
            "content": "Bias is a consistent push in one direction; noise is inconsistent scatter in many directions. If every assessor is five points too high, that’s bias; if half are too high and half too low on the same case, that’s noise. In real organizations you usually have both, but noise hides better because each decision looks sensible in isolation. The cost shows up only when you line up parallel decisions and see spread where sameness is expected. Bias you can debate; noise you must measure, because intuition cannot feel variance. Treat the problem like marksmanship: bias shifts the cluster from the bullseye, noise widens the cluster. You can’t hit the target reliably unless you shrink both."
          },
          {
            "heading": "Three Kinds of Noise: Level, Pattern, Occasion",
            "content": "Level noise is the stable tendency of some judges to be systematically higher or lower than peers across cases (the “tough graders vs. soft graders” effect). Pattern noise is idiosyncrasy—two equally experienced assessors agree on average yet diverge case by case because they notice different cues or weigh them differently. Occasion noise is within-person drift driven by mood, fatigue, priming, context, or time of day; the same judge gives different answers to indistinguishable cases on Tuesday morning vs. Friday afternoon. Total noise is their sum. Knowing which part dominates matters because each calls for different fixes: calibration for level, structure for pattern, and guardrails or batching for occasion."
          },
          {
            "heading": "Why Noise Is Expensive",
            "content": "Noise wastes money, erodes trust, and creates avoidable unfairness. Customers pay different prices for the same risk profile, patients get different tests for the same symptoms, applicants face different odds for the same résumé, and defendants receive different sentences for similar crimes. Leaders experience this as churn—appeals, second opinions, rework, complaints—without realizing the hidden driver is variability that no one has mapped. Because stakeholders cannot predict how they will be treated, they treat the system as a lottery, and lotteries invite both cynicism and gaming. Reducing noise turns reputational heat into operational relief: fewer escalations, faster cycle times, and more defensible outcomes."
          },
          {
            "heading": "Why Noise Is Hard to See",
            "content": "Our brains explain single outcomes with stories, not ensembles with statistics. After a decision, we can always rationalize why it was reasonable: a salient detail, a metaphor that fits, a hunch that resonates. But reasonableness does not equal reliability. Without side-by-side comparisons, spread looks like nuance. That is why organizations can run for years with wide dispersion: there is no regular practice of showing independent judgments on common cases. The cure is an audit mindset—treat variability as a measurable property of the system rather than a personal virtue of the decision maker."
          },
          {
            "heading": "Reliability Before Validity",
            "content": "Validity asks, “Are we right on average?” Reliability asks, “Would we answer the same way again?” A system cannot be valid for individuals if it is unreliable across assessors or days; the same person might be over- or under-served purely by luck of assignment. This is why standardized rubrics, defined scales, and aggregation across independent judges often beat unaided expertise. Reliability is not the enemy of professional judgment; it is the container that lets judgment do useful work without spilling into arbitrariness."
          },
          {
            "heading": "Equity Implications of Noise",
            "content": "Even when a process is unbiased on average, noise creates pockets of unfairness because some groups encounter harsher or more lenient decision makers by chance. Random inequity is still inequity. Moreover, noise makes bias harder to detect: wide scatter can mask a consistent tilt. Teams that care about fairness therefore care about variance as much as about mean differences. The practical payoff is simple: when you tighten variance with structure and aggregation, you reduce both arbitrary harm and the fog that hides systematic problems."
          },
          {
            "heading": "The Seduction of Confidence",
            "content": "Confident explanations make noisy systems look precise. Experts feel certain because each case triggers a coherent story; that story is often different from the one they told yesterday. Confidence tracks narrative fluency more than accuracy or repeatability. The fix is to anchor on external checks: inter-rater agreement on common vignettes, test-retest stability, and forecast scoring against future outcomes. Confidence that survives those checks is the kind you can trust."
          },
          {
            "heading": "Where Noise Lives",
            "content": "Noise thrives wherever tasks require synthesis of soft cues, ambiguous signals, or conflicting goals: underwriting, diagnosing, grading essays, performance reviews, sentencing, policy triage, creative commissioning. It shrinks where tasks are decomposed into defined sub-judgments with clear scales and later re-combined, or where algorithms perform parts that humans perform inconsistently. The aim is not to mechanize everything but to reserve human attention for places where variation equals value—empathy, negotiation, meaning—while removing variation where it equals error."
          },
          {
            "heading": "The North Star: Decision Hygiene",
            "content": "The central promise of this course is that you can cut error without first diagnosing whether it is bias or noise. Decision hygiene routines—independent mediating assessments, defined scales, blinding, aggregation—lower both at once by shrinking discretion at the wrong moments and focusing judgment at the right ones. Think of hygiene as checklists for thinking: boring when read aloud, transformative when practiced. In the remaining units, you will learn how to measure noise, redesign judgments, and prove that the new process holds its shape when the pressure rises."
          }
        ]
      },
      "unit-2-measuring-noise-audits": {
        "display_order": 20,
        "name": "Unit 2: Measuring Noise (Noise Audits)",
        "display_name": "Unit 2",
        "paragraphs": [
          {
            "heading": "What a Noise Audit Is",
            "content": "A noise audit is a structured experiment on your own judgment process. Multiple qualified assessors independently evaluate the same anonymized set of cases using the current method. You then quantify how far those assessments spread for identical inputs. The audit does not blame individuals; it characterizes the system. The output is a map: overall dispersion, how much stems from level vs. pattern vs. occasion effects, and where spread clusters by case type. With that map you can prioritize fixes and predict the benefit of hygiene routines before rolling them out."
          },
          {
            "heading": "Sampling the Right Cases",
            "content": "Pick a case set that represents your real workload: a balanced mix across difficulty, stakes, and edge conditions, typically 20–50 items for a first pass. Ensure cases are sufficiently anonymized to remove irrelevant cues (names, photos, dates) while preserving the information experts legitimately use. Include a few duplicates to estimate within-person occasion noise and a few gold-standard anchors (e.g., previously adjudicated cases) to check calibration. If seasonality affects judgments, sample across time so you do not audit a thin slice of reality."
          },
          {
            "heading": "Independence and Blinding",
            "content": "Independence is the audit’s oxygen. Judges should work alone, in parallel, without seeing each other’s notes, interim results, or summary statistics. Remove fields that leak peer decisions (prior scores, legacy labels) and randomize case order per rater. If practical, separate the rating phase from any subsequent discussion or consensus meeting by a cooling-off period. Without independence you are measuring conformity pressure or anchoring, not genuine dispersion, and your variance estimate will be falsely low."
          },
          {
            "heading": "Scales, Rubrics, and Mediating Assessments",
            "content": "Define what is being judged in behavioral terms and specify scales before rating begins. Replace free-form global judgments with independent mediating assessments (IMAs): sub-ratings on the key dimensions that, when combined, yield the overall decision. For example, an underwriting score might independently rate base risk, volatility, documentation quality, and mitigation evidence. Provide concrete anchors for scale points (what a 2 vs. a 4 looks like) and a short checklist of mandatory cues per dimension. IMAs make disagreement legible: you can see whether spread arises from different weights or different readings of the same cue."
          },
          {
            "heading": "How to Quantify Spread",
            "content": "Report dispersion in units decision makers understand. For continuous scores, use standard deviation, interquartile range (IQR), and median absolute deviation (MAD). For categorical outcomes, compute pairwise disagreement rates and the share of cases where the most lenient and most stringent decisions differ (the ‘range of consequence’). Convert abstract metrics into lived units: dollars of premium spread, months of sentence spread, points of grade spread. Include both overall dispersion and dispersion by segment (case type, complexity, assessor cohort) so leaders can see where noise hurts most."
          },
          {
            "heading": "Separating Level, Pattern, and Occasion Noise",
            "content": "To decompose noise, fit a simple random-effects model or, if tooling is limited, use intuitive summaries. Level noise: average each assessor’s scores and compare to the grand mean; stable offsets indicate toughness/leniency. Pattern noise: after subtracting level offsets, examine residual disagreement on a case-by-case basis. Occasion noise: compare an assessor to themselves on duplicated cases separated in time. Even a rough decomposition is actionable: calibration closes level gaps; structure and IMAs close pattern gaps; batching, breaks, and timing policies damp occasion effects."
          },
          {
            "heading": "Visualizing Noise for Decision Makers",
            "content": "Make variance visceral. Dot plots with one dot per assessor per case show scatter at a glance; fan charts display how uncertainty grows across sub-judgments; violin plots reveal distribution shape. Overlay the organization’s tolerance bands—how much spread is acceptable before fairness or cost is compromised. Visuals should also show before/after when you pilot hygiene routines, so executives can see shrinkage rather than just read about it. A one-page dashboard with three plots usually beats ten pages of tables."
          },
          {
            "heading": "Linking Dispersion to Outcomes",
            "content": "Noise matters because it changes real results. Where you can, connect spread to downstream costs: rework rates, appeal rates, churn, loss ratios, readmissions, complaint volume. Show two numbers: the current dispersion and the modeled savings if you cut that dispersion by one-third. If gold-standard anchors exist, report calibration error alongside noise: a system can be both variable and off-target, and leaders should not celebrate consistency if it nails the wrong value. Tying variance to money, risk, or harm turns curiosity into mandate."
          },
          {
            "heading": "Turning Audit Findings into a Plan",
            "content": "Every audit should end with a shortlist of interventions and a test plan. If level noise dominates, run calibration sessions with immediate feedback and publish personal offsets. If pattern noise dominates, redesign the judgment into IMAs with anchored scales and require a brief justification per dimension. If occasion noise dominates, batch similar cases, schedule high-stakes work for protected times, and insert micro-breaks. Pre-register what ‘success’ means (e.g., 30% IQR reduction without loss of validity), pilot on one unit, and schedule a follow-up audit to verify durability."
          },
          {
            "heading": "Common Pitfalls",
            "content": "Typical failure modes include leaking peer signals (which collapses independence), auditing only easy cases (which understates spread), mixing calibration and consensus (which hides disagreement), and celebrating lower dispersion that comes from anchoring rather than from better reading of evidence. Avoid them by keeping phases separate, sampling broadly, documenting methodological choices, and preserving raw, rater-level data for replication. A good audit is modest, transparent, and repeatable—the opposite of a one-off performance."
          }
        ]
      },
      "unit-3-psychology-of-inconsistency": {
        "display_order": 30,
        "name": "Unit 3: Psychology of Inconsistency",
        "display_name": "Unit 3",
        "paragraphs": [
          {
            "heading": "Heuristics: Fast, Useful—and Varied",
            "content": "Heuristics compress complex judgments into quick cues: representativeness for probability, availability for frequency, affect for value. They work because most environments are regular enough that shortcuts pay. But because people learn different cues from different histories, and because contexts activate different shortcuts, reliance on heuristics produces dispersion. Two experienced reviewers can read the same résumé yet one foregrounds pedigree while the other foregrounds quantified outcomes. Neither is irrational; they are simply optimizing for different regularities. Recognizing this variability is the first step to channeling it—preserving speed where it helps and curbing it where it scatters decisions."
          },
          {
            "heading": "Attribute Substitution and Hidden Swaps",
            "content": "When a hard question arrives—How risky is this borrower?—the mind silently substitutes an easier one—How typical does this profile look, given my memory? The swap feels natural, so judges are unaware that the criterion shifted. Because the substitute attribute differs across people (one sees debt-to-income, another sees employer stability, a third hears tone), the same case yields different answers. Decision structures that force judges to rate the hard attribute directly, or to rate the substitute attributes separately before combining them, reduce these hidden swaps and the noise they create."
          },
          {
            "heading": "Anchoring and Order Effects",
            "content": "Initial numbers, categories, or exemplars pull later judgments more than we admit. A high first quote can lift an entire negotiation; a harsh first case can make the second feel milder by contrast. In evaluation stacks, early standouts set an anchor that drags the baseline. Order also matters in interviews: asking about leadership before ethics elicits different stories than the reverse. Independence protocols that randomize order, suppress prior scores, and prevent early consensus protect against anchors and sequence artifacts, shrinking occasion noise without slowing work."
          },
          {
            "heading": "Framing and Reference Points",
            "content": "The same facts described as gains or losses produce different choices because the mind codes outcomes relative to a reference point. A policy that “saves 200 lives” feels different from one that “avoids 600 deaths,” even when mathematically equivalent. Reference points vary across judges—budget holders anchor on last year’s spend; clinicians anchor on the sickest recent patient—so frames introduce inter-rater variability. Shared baselines (last quarter’s mean, peer medians) and standard statement templates reduce frame drift and bring judgments onto common ground."
          },
          {
            "heading": "Mood, Fatigue, and Context",
            "content": "Sleep debt, blood sugar, weather, workload, and background noise nudge thresholds up or down. The same person rates one case aggressively before lunch and conservatively during an afternoon slump. Because such nudges are invisible, they are unlikely to be self-corrected. Batching similar cases, scheduling high-stakes work for protected hours, inserting micro-breaks, and using brief self-checks (“Would I make the same call at 9 a.m.?”) damp occasion noise and create more even treatment for identical inputs."
          },
          {
            "heading": "Selective Attention and Cue Weight Drift",
            "content": "Humans cannot attend to all cues; we spotlight what past success taught us to notice. Over time, spotlight positions drift with recency and salience. After a dramatic failure linked to one variable, judges over-weight that variable for weeks; after a long quiet period, they under-weight it. Making cue weights explicit in a rubric, reviewing aggregate outcomes quarterly, and rebalancing weights with data keeps attention aligned with predictive value rather than with whatever most recently shouted in memory."
          },
          {
            "heading": "Complexity and the Illusion of Insight",
            "content": "Adding more information feels like adding accuracy, but beyond a threshold, extra cues create degrees of freedom for stories to diverge. Two assessors facing a rich dossier can each craft coherent but incompatible narratives. The fix is counterintuitive: limit inputs to proven predictors and score them independently before synthesis. Complexity belongs in exploration; parsimony belongs in decisions that must be consistent. When synthesis does occur, require a short written rationale that cites the specific scored cues rather than narrative gloss."
          },
          {
            "heading": "Metacognitive Myopia",
            "content": "People are poor judges of their own variability. Experts remember their best calls and attribute misses to special circumstances; they rarely see the full distribution of their judgments. Providing judges with calibration reports—how often their ‘high’ rating corresponded to high outcomes, how their levels compare to peers, how their test–retest stability looks—improves self-knowledge and moderates extremes. Metacognition is a skill; give it measurements and it grows."
          },
          {
            "heading": "Group Dynamics That Amplify Noise",
            "content": "Paradoxically, groups meant to stabilize decisions can increase dispersion through conformity or polarization. Early talk anchors the room; status hierarchies silence disconfirming cues; and consensus pressure pushes to the middle even when extremes carry evidence. Structured protocols—independent ratings first, then evidence sharing, then a second private rating—convert groups into averaging machines rather than echo chambers. The same people, differently sequenced, produce far less noise."
          },
          {
            "heading": "Where Variation Is Valuable",
            "content": "Not all variability is error. In discovery, design, or negotiation, diverse lenses generate options and expand search. The trick is staging: invite divergence when generating hypotheses and require convergence when making consequential, repeatable calls. Teach teams to label the phase they are in. When people know whether variation is fuel or friction in this moment, they can lean into it or damp it deliberately, rather than letting inconsistency choose for them."
          }
        ]
      },
      "unit-4-decision-hygiene-basics": {
        "display_order": 40,
        "name": "Unit 4: Decision Hygiene Basics",
        "display_name": "Unit 4",
        "paragraphs": [
          {
            "heading": "Hygiene: Reduce Error Without Explaining It First",
            "content": "Decision hygiene is a family of practices that lower both bias and noise without diagnosing which one is present. Like washing hands before knowing the specific pathogen, hygiene shrinks error by constraining bad degrees of freedom and standardizing the steps that precede judgment. Its virtue is practicality: teams can adopt it tomorrow, measure effects next month, and keep iterating regardless of which cognitive story turns out to be true."
          },
          {
            "heading": "Decompose into Independent Mediating Assessments",
            "content": "Force complex calls through a small set of sub-judgments that are scored separately before synthesis—documentation quality, base risk, volatility, mitigation evidence; or severity, reversibility, cost, and benefit. Provide anchors for each scale point and examples of acceptable evidence. IMAs make disagreement legible, prevent global impressions from swamping specifics, and set up aggregation methods that are robust to individual quirks."
          },
          {
            "heading": "Blind What You Can",
            "content": "Hide information that is not causally relevant but is socially or emotionally potent: names, photos, prior scores, institutional pedigree, and peer ratings. Even partial blinding—suppressing legacy labels or removing non-essential narrative fields—reduces anchoring and halo effects. Where full blinding is impossible, delay revealing identity or history until after initial scoring, and record any changes made afterward along with a rationale."
          },
          {
            "heading": "Anchor Scales, Not Stories",
            "content": "Unanchored scales invite drift (“3” means different things to different people and to the same person over time). Build descriptive anchors at key points: what a 1, 3, and 5 look like with examples and thresholds. Include a ‘cannot rate’ option to prevent forced guessing. Periodically recalibrate by having all judges rate a small common set and then discussing only where and why they diverged on the anchors, not on personal preferences."
          },
          {
            "heading": "Average Independent Judgments",
            "content": "Combining multiple, truly independent judgments outperforms most single experts because random errors cancel. Independence is fragile: shared briefings, visible early scores, and discussion before rating collapse it. Sequence the workflow as solo → aggregation → discussion. When stakes are high, consider outlier trimming or weighted averages that give more influence to judges with proven calibration, but guard against extreme weighting that reintroduces single-point failure."
          },
          {
            "heading": "Use Simple, Transparent Rules Where They Win",
            "content": "Mechanical rules—point systems, linear models, scorecards—often match or beat unaided judgment on reliability and accuracy. They shine because they are consistent: the same inputs produce the same outputs every time. Build them with either historical data (regression with cross-validation) or disciplined expert elicitation converted into points. Keep the model sparse, publish the coefficients, and lock the version. Complexity raises variance and reduces trust; parsimony travels."
          },
          {
            "heading": "Calibrate People Like Instruments",
            "content": "If level noise is large, give judges feedback on their personal offsets and hit rates. Show each rater how their distribution compares to peers and to outcomes, then rehearse with rapid-cycle practice cases and immediate feedback. Calibration is not shaming; it is tuning. Celebrate movement toward shared anchors and stability over time, not just mean accuracy. When people see themselves as instruments that can be tuned, variance narrows without bruised egos."
          },
          {
            "heading": "Guardrails for Occasion Noise",
            "content": "Standardize when and how high-stakes judgments are made. Batch similar cases; create protected, interruption-free blocks; enforce micro-breaks; and set quotas that prevent end-of-day rushes. Insert self-check prompts at submission (“Would I make the same call tomorrow morning?”). Automated alerts can flag unusual streaks (e.g., ten declines in a row) and nudge a quick pause. These frictions are small but reliable dampers on within-person scatter."
          },
          {
            "heading": "Record, Review, and Rehearse",
            "content": "Hygiene sticks when it is visible. Require short rationales tied to rubric items, log overrides with reasons, and archive rater-level data for periodic audits. Run monthly mini-audits on a handful of cases to see whether dispersion is creeping back. Rehearse the protocol with new team members and after major policy changes. The aim is not bureaucracy for its own sake; it is memory—so the system remembers what produced reliability when turnover or stress arrives."
          },
          {
            "heading": "Know the Limits and Keep Humanity",
            "content": "Over-structuring can crush signal where variation equals value: empathy in clinical talk, creativity in design, context in negotiation. Use hygiene to standardize the parts of the process that should not vary (evidence collection, scales, aggregation), and deliberately leave room where judgment adds meaning. Tell teams exactly which zones invite discretion and which zones demand discipline. Clarity about the line keeps both consistency and compassion intact."
          }
        ]
      },
      "unit-5-structuring-judgments": {
        "display_order": 50,
        "name": "Unit 5: Structuring Judgments",
        "display_name": "Unit 5",
        "paragraphs": [
          {
            "heading": "From Vibes to Design",
            "content": "Unstructured judgment invites drift: people notice different cues, tell different stories, and land in different places. Structuring a decision does not mean eliminating human insight; it means deciding, in advance, what information matters, how it will be recorded, and how partial assessments will be combined. Begin by stating the decision question in operational terms (“approve, decline, or defer within 48 hours”) and the target outcome you care about (loss rate, readmission, employee success at 12 months). Then list the few dimensions that most plausibly predict that outcome, and commit to measuring them first—before any global rating is allowed. This sequence shrinks degrees of freedom at the wrong moment (when first impressions are loud) and preserves them at the right moment (when trade-offs must be weighed)."
          },
          {
            "heading": "Define Signals and Exclude Noise",
            "content": "A good structure distinguishes between signals (features causally or historically linked to the outcome) and noise (tempting but irrelevant attributes). Write both lists. In hiring, signal might include examples of shipped work and evidence of learning speed; noise might include alma mater prestige or stylistic flourishes unrelated to the job. Put the signal list on the form as required fields and hide the noise list from raters by design—do not show names, photos, or legacy ratings until after sub-scores are captured. When people cannot see irrelevant cues, they cannot anchor on them, and your process becomes reliably blind where it should be."
          },
          {
            "heading": "Independent Mediating Assessments (IMAs)",
            "content": "IMAs are the backbone of structure: small, independent sub-judgments (e.g., base risk, volatility, documentation quality, mitigation evidence) scored before any synthesis. Each IMA has a behavioral scale with anchors (“3 = documentation includes bank statements covering 6 months, reconciled to cash flow; 5 = audited statements with variance analysis”). Because IMAs are independent, disagreement becomes legible—you can see whether divergence comes from reading the same cue differently or privileging different cues. IMAs also enable cleaner aggregation methods (averages, weighted sums, rules) without arguments about global “fit.”"
          },
          {
            "heading": "Anchored, Behavioral Scales",
            "content": "Unanchored 1–5 scales drift. Write anchors that name observable behaviors, thresholds, and examples at two or three points (1, 3, 5). Avoid adjectives (“excellent,” “adequate”) and prefer concrete criteria (“evidence from two independent sources,” “error rate <2% on last 1,000 cases”). Include a “cannot rate” option to prevent forced guessing and require short evidence notes tied to each anchor selection. Periodic calibration sessions should compare anchor use, not personalities, so the scale remains common property rather than five private languages masquerading as one."
          },
          {
            "heading": "Evidence Before Opinion",
            "content": "Sequence matters. Force evidence collection and IMA scoring before any free-text narrative or global recommendation. A short timer helps (“capture IMAs within 10 minutes, then write your summary”). People who summarize first will backfill evidence to match the story; people who score first will summarize patterns grounded in those scores. If your tool permits, hide the global decision button until all required IMAs and notes are complete. Boring friction here pays off as reduced occasion and pattern noise later."
          },
          {
            "heading": "Checklists That Bite",
            "content": "Checklists are not laundry lists; they are minimum viable safety rails. Keep them short (5–9 items), specific (“verified baseline with comparator?”), and tied to failure modes you’ve actually seen (“duplicate identity check complete?”). Require a yes/no and an evidence pointer (file, timestamp, URL). If an item is often skipped, ask why and either remove it or enforce it with tooling (cannot proceed without attachment). The discipline is to prune ruthlessly: a checklist that tries to do everything will do nothing reliably."
          },
          {
            "heading": "De-biasing Through Design, Not Willpower",
            "content": "Training people to ‘try harder’ against bias rarely survives deadlines. Structural nudges do: randomized case order to defeat streak effects; hiding prior labels to prevent anchoring; forcing comparative judgments (rank these three) when absolute scales are vague; and standardizing prompts (“rate severity, reversibility, and cost separately”) to prevent attribute substitution. Each nudge removes a small source of uncontrolled variance. Together they turn a personality-driven process into a system that different people can run and produce similar results."
          },
          {
            "heading": "When Not to Decide",
            "content": "Structure should include an explicit “defer” path. Some cases arrive without enough signal to support a reliable judgment; deciding anyway increases noise. Define deferral criteria (“missing mandatory documents,” “contradictory evidence unresolved,” “model confidence interval crosses threshold”) and the smallest next step with a deadline (request, test, second sample). Publish deferral rates and turnaround so the path cannot be abused to avoid accountability. Counterintuitively, allowing no-decision increases overall consistency because it prevents forced calls on thin evidence."
          },
          {
            "heading": "Versioning and Change Control",
            "content": "Treat your judgment structure like software. Give each form, scale, and aggregation rule a version number. Log changes with a one-line rationale and an effective date. When outcomes shift, you can attribute improvements to specific changes rather than to vibes. Versioning also enables A/B tests—half the team uses v1.3, half uses v1.4—and quick rollbacks if dispersion widens. Without change control, every improvement is anecdote; with it, you can run a learning system."
          },
          {
            "heading": "Make the Structure Teach",
            "content": "A thoughtful form doubles as training. Next to each scale anchor, add hover text with examples; next to each checklist item, link to a one-paragraph SOP; beside each IMA, show the last quarter’s distribution and the two most common evidence errors. New raters ramp faster, veterans converge on common patterns, and managers have a shared surface where coaching connects to artifacts. Structure, then, is not bureaucracy; it is a curriculum embedded in the workflow."
          }
        ]
      },
      "unit-6-aggregation-and-algorithms": {
        "display_order": 60,
        "name": "Unit 6: Aggregation & Algorithms",
        "display_name": "Unit 6",
        "paragraphs": [
          {
            "heading": "Why Combining Beats Lone Experts",
            "content": "Independent judgments contain idiosyncratic errors that cancel when combined. Even simple averages often outperform the best individual because no single rater is consistently superior across all cases. The keys are true independence (ratings made without leakage), sufficient diversity (different lenses), and consistent scales (so numbers mean the same thing). Aggregation shrinks noise mechanically; it also dampens bias if biases differ across raters. Think of it as crowd insurance against any one person’s good or bad day."
          },
          {
            "heading": "Choose the Right Combiner",
            "content": "The mean is a powerful default, but not the only option. Medians resist outliers; trimmed means drop extreme tails; majority voting works for categorical calls; Borda counts help with rankings. For skewed distributions, consider a winsorized mean. Publish your choice and tie it to the tolerance for error: life-and-death decisions may prefer conservative combiners that resist one enthusiastic outlier; cost-allocation decisions may tolerate means to capture marginal information. The rule is less important than its stability and transparency."
          },
          {
            "heading": "Weighting Without Overfitting",
            "content": "It is tempting to weight raters by seniority or confidence. A better approach is to weight by demonstrated calibration: how well a rater’s past scores tracked outcomes after controlling for case mix. Keep weights modest—over-weighting recreates single-point failure—and update them slowly to avoid chasing noise. Confidence can be used as a gating factor (“low confidence triggers a second independent rating”) rather than as a weight. When in doubt, start equal and move only when evidence persists across quarters."
          },
          {
            "heading": "Mechanical Rules Often Win",
            "content": "Simple, transparent models—linear scorecards with a handful of predictors—frequently match or beat unaided experts on accuracy and reliability. They shine because they are consistent: the same inputs produce the same outputs every time. Build them with either historical data (regression with cross-validation) or disciplined expert elicitation converted into points. Keep the model sparse, publish the coefficients, and lock the version. Complexity raises variance and reduces trust; parsimony travels."
          },
          {
            "heading": "Building a Scorecard From IMAs",
            "content": "Your IMAs are ready-made inputs for a rule. Combine them with a weighted sum or simple decision table (“approve if severity ≤2 and mitigation ≥4”). Calibrate thresholds on held-out data and set gray zones that trigger a second human review. Add a sanity check that converts scores into lived units for spot checks (“at this threshold, expected loss rate is 1.8–2.2%”). When humans can reason about the rule in their own terms, they will use it rather than fight it."
          },
          {
            "heading": "Human–Algorithm Hybrids",
            "content": "The best systems reserve human discretion for exceptions and context while letting algorithms standardize repeatable parts. A common pattern is: model screens and proposes; a human reviews only gray-zone or high-stakes cases; overrides are allowed but must cite which scale item the model misread. Keep the override rate visible and audit outcomes of overridden vs. non-overridden cases. If overrides systematically improve results, update the model; if not, tighten the override gate."
          },
          {
            "heading": "Validation and Robustness Checks",
            "content": "Never judge a combiner on the data that trained it. Hold out time-based slices to mimic real deployment, report performance with uncertainty, and compare to a human-only baseline. Stress-test by segment (case type, region, season) and by distribution shift (“what happens when average case severity rises by 20%?”). Favor models that are slightly less accurate but more stable across segments; reliability under drift protects fairness and operational sanity."
          },
          {
            "heading": "Monitoring, Drift, and Recalibration",
            "content": "Inputs and environments change. Set up dashboards that track input distributions, calibration plots (predicted vs. observed), and agreement between model and human aggregates. Pre-register when you will retrain or retune (e.g., quarterly if calibration error exceeds a threshold). Keep an audit trail of changes with reasons and back-tests. Monitoring is not a compliance chore; it is how you keep variance tamed when reality moves the goalposts."
          },
          {
            "heading": "Fairness as a Constraint, Not a Wish",
            "content": "Aggregation and algorithms can reduce arbitrary variability yet still produce disparate impacts. Decide which fairness notions you will check (error rates by group, calibration within groups, equalized thresholds) and publish the trade-offs. Apply the same hygiene: independent audits, held-out validation, and override reviews with group-aware analysis. Practical fixes include group-specific calibration, feature reviews for proxies, and decision-side guardrails that prevent extreme outcomes for any group without evidence."
          },
          {
            "heading": "Governance That Builds Trust",
            "content": "Create a lightweight change board with data owners, domain leads, and an ethics voice. Require: 1) a one-page proposal for any change to aggregation or models; 2) pre/post metrics; 3) a rollback plan; and 4) a communication note for raters. Log overrides, publish quarterly variance and calibration reports, and keep exemplars where the system worked and where it failed. Governance makes the system legible to newcomers and defensible to outsiders—and keeps you from drifting back to vibes."
          }
        ]
      },
      "unit-7-calibration-and-organizational-design": {
        "display_order": 70,
        "name": "Unit 7: Calibration & Organizational Design",
        "display_name": "Unit 7",
        "paragraphs": [
          {
            "heading": "Calibration Is a System Property",
            "content": "Calibration is not a talent some people simply have; it is a property of a judgment system shaped by feedback, measurement, and incentives. A well-calibrated team gives probabilities that match frequencies, levels that match targets, and narratives that match the evidence on record. You get there by making accuracy and stability visible, routinizing practice on known cases, and rewarding improvement. Treat each rater like an instrument you can tune, each rubric like a spec you can update, and each meeting like a chance to close the loop between beliefs and outcomes."
          },
          {
            "heading": "Build a Calibration Set You Can Reuse",
            "content": "Create a living library of benchmark cases with agreed outcomes or gold-standard adjudications. Cover easy, typical, and edge cases; include duplicates for test–retest; and refresh quarterly so the set tracks reality. For each case, store the minimal dossier needed to make the call and the rationale behind the gold standard. This set powers onboarding, monthly drills, tool comparisons, and sanity checks after policy changes. When calibration is anchored to shared artifacts, debates move from opinion to evidence."
          },
          {
            "heading": "Personal Offsets and Reliability Reports",
            "content": "Give each rater a private, recurring report: level offset from team mean, dispersion around self (test–retest), agreement with peers on IMAs, and calibration vs. outcomes where available. Plot trends rather than single snapshots, and highlight small, sustained improvements. Pair the report with one coaching nudge per month (“tighten documentation IMA anchors”) rather than a dozen asks. The goal is steady shrinkage of avoidable spread without bruising judgment or morale."
          },
          {
            "heading": "Calibration Training That Sticks",
            "content": "Run short, frequent drills instead of rare marathons. Protocol: 1) raters independently score 6–10 benchmark cases with IMAs; 2) reveal the gold standard and team distribution; 3) discuss only where anchors were used differently; 4) log one anchor change or example per IMA; 5) repeat next month. Keep drills time-boxed (≤30 minutes) and rotate facilitators. Learning compounds because the structure stays stable and the content—those anchors and examples—gets sharper."
          },
          {
            "heading": "Policy Guardrails and Tolerance Bands",
            "content": "Publish tolerance bands for dispersion and calibration by decision type (e.g., IQR ≤ 1 point on severity; probability calibration error ≤ 5%). When a team or rater crosses a band, trigger an automatic action: extra drill, shadow reviews, or a temporary second-read requirement. Guardrails depersonalize remediation and prevent arguments about whether variation “feels acceptable.” They also give leaders pre-agreed leverage to pause rollouts that would widen noise."
          },
          {
            "heading": "Incentives: Reward Reliability, Not Just Volume",
            "content": "If performance dashboards pay only for throughput or approval rates, you will buy speed and spread. Add reliability metrics to scorecards: agreement on benchmarks, stability on duplicates, adherence to IMAs and checklists, and override quality when applicable. Celebrate teams that improve reliability without sacrificing validity, and make that improvement a path to visible perks—priority tooling, conference slots, or schedule control. People move toward the prestige you measure."
          },
          {
            "heading": "Roles, RACI, and Decision Stewardship",
            "content": "Separate the role that makes a call from the role that stewards the process. A Decision Owner is accountable for outcomes; a Decision Steward owns the rubric, scales, calibration set, and audits. Publish a simple RACI: owners Responsible for decisions; stewards Accountable for reliability; raters Consulted for anchor updates; stakeholders Informed about metrics. This division avoids the common failure where busy owners neglect hygiene because no one is explicitly paid to defend it."
          },
          {
            "heading": "Onboarding for Reliability",
            "content": "New raters should pass through a structured ramp: watch a 10-minute protocol video, complete a mini-calibration on 12 cases, shadow two live sessions, then perform two supervised sessions with feedback on IMAs and rationales. Gate independent work on a basic reliability threshold (e.g., within one point IQR on three key IMAs). Provide a pocket guide: decision question, IMAs with anchors, checklist, and examples of good rationales. Early reliability beats early speed; speed follows once habits are clean."
          },
          {
            "heading": "Documentation and Version Control",
            "content": "Version every rubric, scale, and aggregation rule. Each change gets an ID, date, rationale, and a short pre/post comparison plan. Store decision memos with links to the version in effect. This discipline lets you attribute variance changes to specific edits and roll back if dispersion widens. It also creates a transparent history new team members can read to understand why today’s process looks the way it does."
          },
          {
            "heading": "Org Cadence: The Reliability Rhythm",
            "content": "Set a light but firm rhythm: weekly spot checks (5 cases), monthly calibration drill, quarterly noise audit on a larger sample, and an annual protocol review. Tie budget and headcount asks to demonstrated reductions in dispersion or improvements in calibration. When the organization experiences reliability as a recurring habit—not a crisis response—noise stays small even when staff turns over or workload spikes."
          }
        ]
      },
      "unit-8-implementation-lab": {
        "display_order": 80,
        "name": "Unit 8: Implementation Lab",
        "display_name": "Unit 8",
        "paragraphs": [
          {
            "heading": "Scope and Charter",
            "content": "Pick one decision to improve in eight weeks—narrow, high-volume, measurable. Write a one-page charter: decision question, outcome metric that matters (loss rate, readmission, success at 90 days), current pain (appeals, rework), stakeholders, and constraints. Declare success criteria upfront (e.g., 30% IQR reduction with no decline in calibration vs. outcomes). Securing that clarity early prevents later debates about what “worked.”"
          },
          {
            "heading": "Baseline: Mini Noise Audit",
            "content": "Assemble 30–50 representative cases, anonymize, and have 5–7 raters score independently with the current process. Report dispersion overall and by segment; decompose into level, pattern, and occasion components using simple summaries. Translate variance into lived units (dollars, days, points) and capture two or three exemplar cases that illustrate costly spread. This baseline is your before picture and your north star for measuring change."
          },
          {
            "heading": "Design IMAs and Anchors",
            "content": "Workshop the decision into 3–6 Independent Mediating Assessments with behavioral anchors at 1, 3, and 5. For each IMA, list required evidence and one common mistake to avoid. Draft a short checklist aimed at the top failure modes you saw in the baseline. Keep it spartan: if an item is not tied to observed error, cut it. Version the design (v1.0) and collect sign-off from the Decision Steward and Owner."
          },
          {
            "heading": "Tooling and Data Capture",
            "content": "Update your form or system so IMAs are scored before any global rating, scale anchors are visible inline, rationales are tied to each IMA, and a ‘cannot rate’ option exists. Hide irrelevant cues (names, legacy labels) until after scoring. Ensure rater-level data is stored with timestamps and version IDs. If you cannot modify the core tool quickly, use a lightweight adjunct (worksheet or plug-in) that captures the same fields consistently."
          },
          {
            "heading": "Pilot Design: A/B With Safeguards",
            "content": "Randomly assign incoming cases to Control (current process) or Pilot (new IMAs + checklist + independence rules). Keep sample sizes modest but sufficient for spread estimates; pre-register primary endpoints (dispersion, calibration) and a small set of secondary endpoints (throughput, appeals). Define a gray zone that triggers a second independent review in both arms to ensure fairness while you experiment. Pilots earn trust when they are transparent and reversible."
          },
          {
            "heading": "Training and Change Management",
            "content": "Kick off with a 30-minute live demo of the new flow using two benchmark cases. Provide a pocket guide and a 10-minute self-paced practice set. Frame the pilot as a reliability experiment, not a referendum on expertise. Pair each rater with a buddy for the first week to answer anchor questions. Collect friction notes daily and fix obvious snags fast; momentum is a signal that the system cares about the people using it."
          },
          {
            "heading": "Monitoring Dashboard",
            "content": "Stand up a simple dashboard: dispersion by IMA and overall (IQR, MAD), test–retest stability on duplicates, agreement rates, throughput, and a small calibration panel where outcomes exist. Show control vs. pilot side-by-side with uncertainty bands and the pre-registered tolerance lines. Update weekly. A good dashboard makes reliability tangible and focuses discussion on signal rather than anecdotes."
          },
          {
            "heading": "Overrides and Governance",
            "content": "Allow overrides in the pilot with two rules: cite the specific IMA the rule misread, and record the alternative evidence. Review overrides weekly for pattern and performance. If overrides consistently improve outcomes, consider adjusting anchors or thresholds; if they don’t, tighten the gate. Governance is minimal but real: a change log, a rollback plan, and a short note to stakeholders after each adjustment."
          },
          {
            "heading": "Evaluate and Decide",
            "content": "After 4–6 weeks, freeze the data and run the pre-registered analysis. Did dispersion shrink by the target without harming calibration? Did throughput hold? Are appeals, rework, or complaints trending down? Present results on one page with the baseline plot, pilot vs. control plots, and two exemplars where the new structure clearly prevented a bad call. Decide to roll out, iterate, or stop; whichever you choose, publish the reasoning."
          },
          {
            "heading": "Sustain and Scale",
            "content": "If you roll out, lock the protocol as v1.1, schedule a quarterly mini-audit, and add the decision to the monthly calibration drill. Fold reliability metrics into team scorecards and give public credit for improvements. If you scale to another decision, reuse the playbook: baseline, IMAs, tooling, pilot, dashboard, decision. The Implementation Lab ends when noise reduction becomes a habit your organization can run without a special project."
          }
        ]
      }
    }
  }
}
    </script>
    <!-- 
    ============================================================
    ^^^^^^^^^^^^^^ PASTE YOUR JSON CONTENT ABOVE ^^^^^^^^^^^^^^
    ============================================================
    -->

    <script defer>
    document.addEventListener('DOMContentLoaded', () => {

        const THEMES = {
          'newspaper': { 
            name: 'Newspaper', 
            type: 'classic', 
            isDark: false 
          },
          'emerald-paper': {
            name: 'Emerald Paper', 
            type: 'variable', 
            isDark: false,
            vars: {
              '--bg-gradient': '#f7fbf9', '--text': '#1e2b28', '--accent-text': '#0f7f6d', '--nav-bg': '#e7f3ef',
              '--nav-border': '#86c8ba', '--nav-text': '#1e2b28', '--nav-hover-bg': '#d7ece6', '--nav-active-bg': '#bfe3d9',
              '--nav-active-text': '#1e2b28', '--prose-text': '#1e2b28', '--prose-headings': '#0f7f6d'
            }
          },
          'cyber-blue': {
            name: 'Cyber Night', 
            type: 'variable', 
            isDark: true,
            vars: {
              '--bg-gradient': 'rgb(2, 6, 23)',
              '--text': 'rgb(203, 213, 225)',
              '--accent-text': 'rgb(203, 213, 225)',
              '--nav-bg': 'rgb(30, 41, 59)',
              '--nav-border': 'rgb(51, 65, 85)',
              '--nav-text': 'rgb(203, 213, 225)',
              '--nav-hover-bg': 'rgb(15, 23, 42)',
              '--nav-active-bg': 'rgb(51, 65, 85)',
              '--nav-active-text': '#FFFFFF',
              '--prose-text': 'rgb(203, 213, 225)',
              '--prose-headings': '#FFFFFF'
            }
          },
          'emerald-gold': {
            name: 'Emerald/Gold', 
            type: 'variable', 
            isDark: true,
            vars: {
              '--bg-gradient': 'linear-gradient(135deg,#022c22 0%,#064e3b 60%,#065f46 100%)', '--text': '#ecfdf5',
              '--accent-text': '#f0c475', '--nav-bg': '#064e3b', '--nav-border': '#d4a24c', '--nav-text': '#ecfdf5',
              '--nav-hover-bg': '#065f46', '--nav-active-bg': '#0b3d31', '--nav-active-text': '#f0c475',
              '--prose-text': '#a7f3d0', '--prose-headings': '#f0c475'
            }
          }
        };

        const app = {
            root: document.documentElement,
            header: document.getElementById('app-header'),
            tabs: document.getElementById('app-tabs'),
            panels: document.getElementById('app-panels'),
            settings: {
                modal: document.getElementById('settingsModal'),
                closeBtn: document.getElementById('closeSettings'),
                themeSelect: document.getElementById('themeSelect')
            }
        };

        const state = { activeTab: null, theme: 'newspaper' };

        const loadState = () => {
            state.theme = localStorage.getItem('appTheme') || 'newspaper';
            state.activeTab = localStorage.getItem('activeTab');
        };

        const saveState = () => {
            localStorage.setItem('appTheme', state.theme);
            localStorage.setItem('activeTab', state.activeTab);
        };

        const applyTheme = (themeKey) => {
            const theme = THEMES[themeKey];
            if (!theme) return;

            app.root.className = '';
            app.root.style.cssText = '';

            if (theme.type === 'variable' && theme.vars) {
                for (const [key, value] of Object.entries(theme.vars)) {
                    app.root.style.setProperty(key, value);
                }
                app.root.classList.add(`theme-${themeKey}`);
            }

            state.theme = themeKey;
            if (app.settings.themeSelect.value !== themeKey) {
                app.settings.themeSelect.value = themeKey;
            }
            saveState();
        };

        const populateThemeSelector = () => {
            app.settings.themeSelect.innerHTML = Object.entries(THEMES).map(([key, theme]) => 
                `<option value="${key}">${theme.name}</option>`
            ).join('');
        };

        const loadAndTransformData = () => {
            try {
                const dataNode = document.getElementById('app-data');
                if (!dataNode.textContent.trim()) {
                    console.error("JSON data is empty.");
                    return null;
                }
                const rawData = JSON.parse(dataNode.textContent);
                
                const topLevelKey = Object.keys(rawData)[0];
                if (!topLevelKey) {
                    console.error("JSON data is missing a top-level key.");
                    return null;
                }
                const course = rawData[topLevelKey];
                
                const units = Object.entries(course.categories).map(([id, unitData]) => ({ id, ...unitData }))
                    .sort((a, b) => a.display_order - b.display_order);
                return { meta: { title: course.title, description: course.description }, units };
            } catch (e) {
                console.error("Failed to load or transform app data:", e);
                app.panels.innerHTML = `<div class="p-4 text-red-500 bg-red-100 rounded"><strong>Error:</strong> Could not load content. Please ensure the JSON is correctly formatted and pasted into the data island.</div>`;
                return null;
            }
        };
        
        const data = loadAndTransformData();
        
        if (!data) return;

        const renderHeader = () => {
            document.title = data.meta.title || "Interactive Course";
            
            app.header.innerHTML = `
                <div class="flex justify-between items-start gap-4">
                    <div>
                        <h1 class="text-3xl font-bold text-accent">${data.meta.title}</h1>
                        <p class="mt-2 text-primary/80">${data.meta.description}</p>
                    </div>
                    <button id="settingsBtn" class="flex-shrink-0 p-3 rounded-lg settings-button transition-opacity" aria-label="Display Settings">
                        <span class="text-2xl text-accent">⚙️</span>
                    </button>
                </div>`;
        };
        
        const renderTabs = () => {
            app.tabs.innerHTML = `<div class="flex flex-wrap">${
                data.units.map(unit => `
                    <button role="tab" aria-selected="false" aria-controls="${unit.id}-panel" id="${unit.id}-tab"
                            class="px-5 py-3 font-semibold transition-colors tab-button">
                        ${unit.display_name}
                    </button>`
                ).join('')
            }</div>`;
        };

        const renderPanels = () => {
            app.panels.innerHTML = data.units.map(unit => {
                const introHTML = unit.introduction ? `<p class="lead">${unit.introduction}</p>` : '';
                const paragraphsHTML = (unit.paragraphs || []).map(p => `
                    <h3>${p.heading}</h3>
                    <p>${p.content}</p>
                `).join('');
                const proseClasses = THEMES[state.theme].isDark ? 'prose prose-lg max-w-none prose-invert' : 'prose prose-lg max-w-none';
                return `
                    <div role="tabpanel" id="${unit.id}-panel" aria-labelledby="${unit.id}-tab" hidden>
                        <div class="${proseClasses}">
                            <h2>${unit.name}</h2>
                            ${introHTML}
                            ${paragraphsHTML}
                        </div>
                    </div>`;
            }).join('');
        };

        const switchTab = (tabId) => {
            if (!data.units.some(u => u.id === tabId)) {
                tabId = data.units[0]?.id;
            }
            if (!tabId) return;

            app.tabs.querySelectorAll('[role="tab"]').forEach(tab => {
                const isActive = tab.id === `${tabId}-tab`;
                tab.setAttribute('aria-selected', isActive);
                tab.classList.toggle('tab-active', isActive);
            });

            app.panels.querySelectorAll('[role="tabpanel"]').forEach(panel => {
                panel.hidden = panel.id !== `${tabId}-panel`;
            });
            
            state.activeTab = tabId;
            saveState();
        };

        const setupEventListeners = () => {
            app.tabs.addEventListener('click', e => {
                const tab = e.target.closest('[role="tab"]');
                if (tab) switchTab(tab.id.replace('-tab', ''));
            });

            app.header.addEventListener('click', e => {
                if (e.target.closest('#settingsBtn')) {
                    app.settings.modal.classList.remove('hidden');
                    app.settings.themeSelect.focus();
                }
            });
            
            const closeModal = () => {
                app.settings.modal.classList.add('hidden');
                document.getElementById('settingsBtn')?.focus();
            };
            app.settings.closeBtn.addEventListener('click', closeModal);
            app.settings.modal.addEventListener('keydown', e => {
                if (e.key === 'Escape') closeModal();
            });

            app.settings.themeSelect.addEventListener('change', (e) => {
                applyTheme(e.target.value);
                renderPanels();
                const activePanel = document.getElementById(`${state.activeTab}-panel`);
                if(activePanel) activePanel.hidden = false;
            });
        };

        const init = () => {
            loadState();
            populateThemeSelector();
            applyTheme(state.theme);
            renderHeader();
            renderTabs();
            renderPanels();
            setupEventListeners();
            switchTab(state.activeTab || data.units[0]?.id);
        };

        init();
    });
    </script>
</body>
</html>

