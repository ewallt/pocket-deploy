<!DOCTYPE html>
<html lang="en">
<head><meta charset="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/>
<title>Neural Topic Models — Part 5: Evaluation & Pitfalls</title>
<style>:root{--bg:#0f1216;--panel:#14181d;--ink:#e6e8eb;--muted:#9aa7b3;--brand:#8aa0b3;--border:#222831}@media(prefers-color-scheme:light){:root{--bg:#f7f5f1;--panel:#ffffff;--ink:#262a2f;--muted:#5b6d7a;--brand:#5b6d7a;--border:#e6e2da}}*{box-sizing:border-box}body{margin:0;background:var(--bg);color:var(--ink);font:16px/1.6 system-ui,-apple-system,Segoe UI,Inter,Arial;padding:2rem}.c{max-width:880px;margin:0 auto;background:var(--panel);border:1px solid var(--border);border-radius:14px;box-shadow:0 6px 24px rgba(0,0,0,.25)}.nav{display:flex;gap:.5rem;flex-wrap:wrap;padding:.75rem;border-bottom:1px solid var(--border)}.nav a{padding:.45rem .7rem;border:1px solid var(--border);border-radius:999px;text-decoration:none;color:var(--ink)}.nav a:hover{border-color:var(--brand);color:var(--brand)}.h{padding:1.25rem 1.5rem}.intro{color:var(--muted);font-style:italic;margin:.25rem 0 1rem}.p{padding:0 1.5rem 1.5rem}.p p{margin:1.1rem 0}.f{padding:1rem 1.5rem;border-top:1px solid var(--border);color:var(--muted);font-size:.95rem}</style></head>
<body><article class="c">
<nav class="nav"><a href="/pocket-deploy/dashboard-neural-topic-models-1/">← Dashboard</a><a href="/pocket-deploy/neural-topic-models-1-4/">← Previous</a></nav>
<header class="h"><h1>Evaluation & Pitfalls</h1><p class="intro">Coherence and diversity matter, but stability, interpretability, and downstream utility decide success.</p></header>
<section class="p">
<p>Topic quality is multidimensional. Practitioners care that top-word lists make sense (coherence), that topics are not near duplicates (diversity), that document assignments are robust (stability), and that features help real tasks. Automated coherence scores include UMass and UCI PMI variants, as well as normalized pointwise mutual information (NPMI), each with different reference corpora and windowing assumptions. Diversity is often computed as the fraction of unique top words across topics. None of these alone substitutes for expert review; they are proxies to steer model selection and hyperparameter tuning.</p>
<p>Human-in-the-loop protocols complement metrics. Word-intruder tests ask raters to spot an off-topic word among top-k candidates; topic-intruder tests offer a mismatched topic for a document and measure detection accuracy. Annotation studies rate interpretability or usefulness on Likert scales in the context of a specific analysis task. These methods are slower but reveal failure modes that automated metrics miss, such as subtle redundancy, cultural bias in word lists, or topics that mix terms from distinct senses of the same word. Combining small human studies with automated sweeps is a pragmatic compromise.</p>
<p>Neural training introduces failure modes beyond classical models. Posterior collapse can make proportions uninformative; mitigate it with KL warm-up, word or input dropout, and decoder capacity constraints. Topic collapse merges many documents into one or two dominant themes; entropy and diversity regularizers help spread mass. Over-smoothed decoders produce vague, generic topics; products-of-experts or sparsity penalties can sharpen them. Conversely, over-sharp decoders fragment broad themes into brittle shards; temperature control, larger batch sizes, and vocabulary cleanup counteract this tendency.</p>
<p>Preprocessing shapes outcomes as much as modeling. Remove stopwords and boilerplate, cap vocabulary size, and standardize tokenization across corpora. Consider bigrams or phrase mining for collocations that carry meaning (“climate change”, “neural network”). For multilingual or domain-shifted data, align embeddings or share decoders while conditioning priors on language or domain tags. Keep train/validation splits document-wise to avoid leakage. Because topic models are sensitive to random seeds, report averages over multiple runs and include stability diagnostics such as topic alignment scores between runs.</p>
<p>Evaluation should be task-aware. If topics feed search or recommendation, measure ranking quality. If they support exploratory analysis, validate with user studies that mirror the dashboard workflows people will actually use. In supervised settings, use topics as features and check whether they improve calibration, fairness, or sample efficiency compared to bag-of-words or dense embeddings. Interpretability sometimes trades off with downstream performance; make that trade explicit and decide based on the application’s constraints and review burden.</p>
<p>Reproducibility and governance close the loop. Log seeds, software versions, and data snapshots; freeze the vocabulary and decoder when publishing dashboards for longitudinal monitoring. Document known biases and limitations, especially when topics may influence decisions. Provide clear affordances for analysts to rename, merge, or hide topics without retraining. With these practices, neural topic models can move from promising demos to dependable tools that help people make sense of large text collections over time.</p>
</section>
<footer class="f">Sources: “Topic Modelling Meets Deep Neural Networks: A Survey” (evaluation metrics; human studies; common failure modes).</footer>
</article></body>
</html>
