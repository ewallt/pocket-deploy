<!DOCTYPE html>
<html lang="en">
<head><meta charset="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/>
<title>Neural Topic Models — Part 3: Neural Dirichlet & ProdLDA</title>
<style>:root{--bg:#0f1216;--panel:#14181d;--ink:#e6e8eb;--muted:#9aa7b3;--brand:#8aa0b3;--border:#222831}@media(prefers-color-scheme:light){:root{--bg:#f7f5f1;--panel:#ffffff;--ink:#262a2f;--muted:#5b6d7a;--brand:#5b6d7a;--border:#e6e2da}}*{box-sizing:border-box}body{margin:0;background:var(--bg);color:var(--ink);font:16px/1.6 system-ui,-apple-system,Segoe UI,Inter,Arial;padding:2rem}.c{max-width:880px;margin:0 auto;background:var(--panel);border:1px solid var(--border);border-radius:14px;box-shadow:0 6px 24px rgba(0,0,0,.25)}.nav{display:flex;gap:.5rem;flex-wrap:wrap;padding:.75rem;border-bottom:1px solid var(--border)}.nav a{padding:.45rem .7rem;border:1px solid var(--border);border-radius:999px;text-decoration:none;color:var(--ink)}.nav a:hover{border-color:var(--brand);color:var(--brand)}.h{padding:1.25rem 1.5rem}.intro{color:var(--muted);font-style:italic;margin:.25rem 0 1rem}.p{padding:0 1.5rem 1.5rem}.p p{margin:1.1rem 0}.f{padding:1rem 1.5rem;border-top:1px solid var(--border);color:var(--muted);font-size:.95rem}</style></head>
<body><article class="c">
<nav class="nav"><a href="/pocket-deploy/dashboard-neural-topic-models-1/">← Dashboard</a><a href="/pocket-deploy/neural-topic-models-1-2/">← Previous</a><a href="/pocket-deploy/neural-topic-models-1-4/">Next →</a></nav>
<header class="h"><h1>Neural Dirichlet & ProdLDA</h1><p class="intro">Logistic–normal priors make VAEs workable; product-of-experts decoders sharpen topics and reduce redundancy.</p></header>
<section class="p">
<p>Classical LDA uses a Dirichlet prior over document–topic proportions because it lives on the simplex and yields convenient conjugacy. Neural training, however, leans on the reparameterization trick for low-variance gradients. The Dirichlet does not admit a simple reparameterization, so most neural topic models replace it with a Gaussian in ℝ<sup>K</sup> pushed through softmax, producing a logistic–normal prior on the simplex. This seemingly minor change is pivotal: it unlocks stable stochastic variational learning, permits covariance structure between topics, and allows metadata-conditioned priors that tilt proportions without redesigning inference.</p>
<p>Dirichlet-like behavior can be reintroduced by shaping the Gaussian. Diagonal covariances mimic symmetric Dirichlets, while full covariances capture correlations such as “economics” co-appearing with “policy.” Sparsity is encouraged by temperature-controlled softmax, Laplace or ℓ<sub>1</sub> penalties on latent means, or stick-breaking-style transforms. Because the encoder amortizes posterior parameters, these inductive biases apply to every document consistently. The result is a flexible prior family that remains differentiable, data-efficient, and easy to regularize, avoiding brittle collapsed-variational updates while retaining interpretable mixture semantics for analysts.</p>
<p>ProdLDA reframes the decoder. Instead of mixing topic distributions additively, it uses a product-of-experts (PoE) view where each topic “votes” by down-weighting off-topic words. Geometrically, mixtures average; products intersect. Intersections produce sharper, more specific word distributions, often improving topic coherence and reducing overlap between neighboring topics. In practice, the decoder computes logit contributions per topic and sums them, which corresponds to multiplying normalized distributions in probability space. This simple architectural choice discourages diffuse, redundant topics and makes high-precision themes more common without adding complex attention mechanisms.</p>
<p>Training ProdLDA resembles a standard VAE: maximize the ELBO over document bags-of-words, with KL annealing to prevent latents from being ignored. Word dropout, batch-norm in the encoder, and careful vocabulary pruning guard against degenerate solutions where a few frequent words dominate every topic. Because PoE sharpens distributions, it can be paired with entropy regularizers on the topic–word matrix to avoid pathological peaky topics. Empirically, practitioners report better coherence at similar perplexity, a reminder that reconstruction fit and human interpretability are related but distinct optimization targets.</p>
<p>Neural “Dirichlet” approximations and PoE decoders combine naturally. The logistic–normal prior provides smooth gradients and expressive covariance; the product decoder enforces agreement among active topics. Together they yield sparse proportions, crisp top-word lists, and scalable inference. The approach also plays well with semi-supervision: labels can nudge priors or add auxiliary classification heads, aligning topics with downstream tasks without losing their unsupervised utility. Compared to collapsing Gibbs samplers, inference is orders of magnitude faster at serve time because a single encoder pass replaces many local updates.</p>
<p>Limitations remain. Products can oversharpen, fragmenting broad themes into brittle subtopics; temperature control and diversity regularizers help. Logistic–normal priors may prefer Gaussian structure that misses truly heavy-tailed behavior; mixtures of Gaussians or normalizing flows can restore flexibility at extra cost. Finally, tuning KL schedules and learning rates is more art than science. Even so, the “logistic–normal + PoE” recipe has become a sturdy baseline for modern topic modeling, striking a useful balance between interpretability and the practicalities of end-to-end neural training.</p>
</section>
<footer class="f">Sources: “Topic Modelling Meets Deep Neural Networks: A Survey” (logistic–normal priors; ProdLDA/product-of-experts).</footer>
</article></body>
</html>
