<!-- /pocket-deploy/dashboard-ai-s03/index.html -->
<!DOCTYPE html>
<html lang="en">
<head><meta charset="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/>
<title>AI — Series s03 Dashboard</title>
<style>
:root{--bg:#0b1020;--panel:#0f172a;--ink:#e5e7eb;--muted:#94a3b8;--brand:#60a5fa;--border:#1f2940}
@media(prefers-color-scheme:light){:root{--bg:#f5f7fb;--panel:#fff;--ink:#1b2a41;--muted:#34495e;--brand:#2563eb;--border:#e3e9f3}}
*{box-sizing:border-box}body{margin:0;background:var(--bg);color:var(--ink);font:16px/1.6 system-ui,-apple-system,Segoe UI,Inter,Arial;padding:2rem}
.c{max-width:880px;margin:0 auto;background:var(--panel);border:1px solid var(--border);border-radius:14px;box-shadow:0 6px 24px rgba(0,0,0,.25)}
.top{padding:.75rem 1rem;border-bottom:1px solid var(--border);text-align:right}.top a{color:var(--muted);text-decoration:none}.top a:hover{color:var(--brand)}
.h{padding:1.4rem 1.6rem;border-bottom:1px solid var(--border)}.h h1{margin:0;font-weight:700;font-size:1.55rem}.h p{margin:.5rem 0 0;color:var(--muted)}
.g{display:grid;gap:1rem;padding:1.25rem}
.a{display:block;padding:1rem 1.2rem;border:1px solid var(--border);border-radius:12px;text-decoration:none;color:var(--ink);background:rgba(255,255,255,.02)}
.a h3{margin:.2rem 0 .4rem;font-size:1.08rem;color:var(--brand)}.a p{margin:0;color:var(--muted)}
.a:hover{border-color:var(--brand);transform:translateY(-1px);box-shadow:0 8px 20px rgba(37,99,235,.15)}
</style></head>
<body>
<main class="c">
  <div class="top"><a href="/pocket-deploy/dashboard-library-ai/">← Library</a></div>
  <header class="h"><h1>Series s03 — 2021–2023: Multimodality & Diffusion</h1><p>From CLIP-style joint embeddings to latent diffusion and instruction-following image models.</p></header>
  <section class="g">
    <a class="a" href="/pocket-deploy/ai-s03-1/"><h3>1 — The Multimodal Turn</h3><p>Why language–vision alignment unlocked flexible perception and control.</p></a>
    <a class="a" href="/pocket-deploy/ai-s03-2/"><h3>2 — Diffusion Outpaces GANs</h3><p>Noise schedules, latent spaces, and the stability advantage.</p></a>
    <a class="a" href="/pocket-deploy/ai-s03-3/"><h3>3 — Instruction Images</h3><p>Text-to-image systems shaped by prompts, datasets, and safety layers.</p></a>
    <a class="a" href="/pocket-deploy/ai-s03-4/"><h3>4 — Retrieval & Tool Use</h3><p>Grounding generation with external knowledge and utilities.</p></a>
    <a class="a" href="/pocket-deploy/ai-s03-5/"><h3>5 — Toward Video & 3D</h3><p>Temporal coherence, diffusion for video, and neural fields.</p></a>
  </section>
</main>
</body>
</html>
