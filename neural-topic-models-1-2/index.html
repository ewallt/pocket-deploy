<!DOCTYPE html>
<html lang="en">
<head><meta charset="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/>
<title>Neural Topic Models — Part 2: VAEs as Topic Models</title>
<style>:root{--bg:#0f1216;--panel:#14181d;--ink:#e6e8eb;--muted:#9aa7b3;--brand:#8aa0b3;--border:#222831}@media(prefers-color-scheme:light){:root{--bg:#f7f5f1;--panel:#ffffff;--ink:#262a2f;--muted:#5b6d7a;--brand:#5b6d7a;--border:#e6e2da}}*{box-sizing:border-box}body{margin:0;background:var(--bg);color:var(--ink);font:16px/1.6 system-ui,-apple-system,Segoe UI,Inter,Arial;padding:2rem}.c{max-width:880px;margin:0 auto;background:var(--panel);border:1px solid var(--border);border-radius:14px;box-shadow:0 6px 24px rgba(0,0,0,.25)}.nav{display:flex;gap:.5rem;flex-wrap:wrap;padding:.75rem;border-bottom:1px solid var(--border)}.nav a{padding:.45rem .7rem;border:1px solid var(--border);border-radius:999px;text-decoration:none;color:var(--ink)}.nav a:hover{border-color:var(--brand);color:var(--brand)}.h{padding:1.25rem 1.5rem}.intro{color:var(--muted);font-style:italic;margin:.25rem 0 1rem}.p{padding:0 1.5rem 1.5rem}.p p{margin:1.1rem 0}.f{padding:1rem 1.5rem;border-top:1px solid var(--border);color:var(--muted);font-size:.95rem}</style></head>
<body><article class="c">
<nav class="nav"><a href="/pocket-deploy/dashboard-neural-topic-models-1/">← Dashboard</a><a href="/pocket-deploy/neural-topic-models-1-1/">← Previous</a><a href="/pocket-deploy/neural-topic-models-1-3/">Next →</a></nav>
<header class="h"><h1>VAEs as Topic Models</h1><p class="intro">Variational autoencoders give topic models an encoder–decoder backbone and a tractable training objective.</p></header>
<section class="p">
<p>A variational topic model casts documents as generated from continuous latent variables that determine their topic proportions. Concretely, we sample a Gaussian vector, transform it through a softmax to the simplex, and treat the result as mixture weights over topics. Words are then drawn from a decoder conditioned on those proportions, typically a linear layer producing a distribution over the vocabulary. This mirrors LDA’s document-topic and topic-word structure while replacing the Dirichlet with a logistic-normal prior, letting us apply the reparameterization trick to learn with stochastic gradient descent.</p>
<p>Training proceeds by maximizing the evidence lower bound (ELBO), which balances data fit against keeping the approximate posterior close to the prior. An encoder network reads a document’s bag-of-words and outputs the mean and variance of a Gaussian that approximates the posterior over latent topic variables. We sample latents through a differentiable path, push them through softmax to get topic proportions, and decode words. The KL term regularizes the encoder; the reconstruction term rewards predicting observed counts. This objective turns inference from bespoke variational updates into standard backprop through a neural net.</p>
<p>Posterior collapse is a common failure mode when the decoder becomes too expressive relative to the encoder, allowing it to explain words without relying on the latent proportions. KL annealing, word dropout, and constraining the decoder to a product-of-experts form can help. The product-of-experts view sharpens topics by intersecting evidence from multiple components, often yielding crisper, less redundant themes. Careful batching, vocabulary pruning, and temperature control for the softmax transformation further stabilize training on large, sparse corpora where gradients can be noisy.</p>
<p>Interpretable topics remain central, so we inspect the decoder’s topic-word matrix to read top words and diagnose redundancy. Encouraging sparsity—via priors on the Gaussian, entropic penalties, or non-negativity constraints on decoder weights—can improve readability without breaking differentiability. Because everything is trained jointly, supervision signals such as document labels or metadata can be injected as auxiliary losses or by conditioning the prior, aligning latent structure with downstream needs while preserving the unsupervised backbone for exploratory analysis.</p>
<p>Neural encoders let us blend counts with semantics. A simple approach feeds pure bag-of-words into the encoder, but richer variants concatenate or fuse contextual embeddings, letting the encoder disambiguate polysemous terms and cluster documents more meaningfully. The decoder can stay linear to keep topics human-readable while the encoder captures non-linear relationships. This division of labor delivers the best of both worlds: fast amortized inference with semantic awareness and a transparent topic-word representation that analysts can actually audit and explain.</p>
<p>Finally, VAEs make extensibility straightforward. Hierarchical priors create topic trees; dynamic priors evolve proportions across time; multilingual variants tie topics across languages with shared decoders or aligned embeddings. Each extension reuses the same ELBO machinery, swapping in a different prior or input representation. In this way, the VAE lens is less a single model than a design pattern for topic modeling in the deep-learning era, unifying classic interpretability with modern training pipelines and deployment realities.</p>
</section>
<footer class="f">Sources: “Topic Modelling Meets Deep Neural Networks: A Survey” (foundations; variational topic modeling).</footer>
</article></body>
</html>
